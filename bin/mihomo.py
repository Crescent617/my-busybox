#!/usr/bin/env python3

import json
import logging
import os
import re
import shutil
import subprocess
import sys
import tempfile
import urllib.parse
import urllib.request
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import Literal

from minifire import fire_like

# ==== CONFIG ====
PROXY_SELECTOR = re.compile(r"é¦™æ¸¯|æ–°åŠ å¡|å°æ¹¾|æ—¥æœ¬")
TARGET_GROUP = "ğŸ”°å›½å¤–æµé‡"

# ==== LOGGING ====
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
logger = logging.getLogger(__name__)

# ==== CONSTANTS ====
MIHOMO_DIR = Path(os.getenv("MIHOMO_DIR") or "~/.config/mihomo").expanduser()
MIHOMO_DIR.mkdir(parents=True, exist_ok=True)
DEFAULT_CONFIG = (Path(__file__).parent / "data" / "mihomo_default.yaml").read_text()
CUSTOM_RULES = (Path(__file__).parent / "data" / "mihomo_rules.yaml").read_text()
CLASH_HOST = "http://0.0.0.0:9090"  # Clash æ§åˆ¶å°åœ°å€
SECRET = ""  # å¦‚æœæœ‰ secretï¼Œå¡«å…¥ï¼Œæ¯”å¦‚ "abc123"
REQUIRED_EXE = ["mihomo", "yq", "git"]


def run_cmd(cmd: str, check=True):
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    if check and result.returncode != 0:
        logger.error(f"å‘½ä»¤æ‰§è¡Œå¤±è´¥: {cmd}")
        logger.error(result.stderr)
        sys.exit(1)
    return result.stdout.strip()


def download_config(sub_url: str):
    if not sub_url:
        logger.error("è¯·è®¾ç½® SUB_URL")
        sys.exit(1)

    logger.info("ä¸‹è½½è®¢é˜…ä¸­...")

    config_file = MIHOMO_DIR / "config.yaml"

    with tempfile.TemporaryDirectory() as temp_dir:
        p = Path(temp_dir)

        sub_config = p / "sub.yaml"
        default_config = p / "config_default.yaml"
        rules_file = p / "rules.yaml"

        default_config.write_text(DEFAULT_CONFIG)
        rules_file.write_text(CUSTOM_RULES)

        try:
            urllib.request.urlretrieve(sub_url, sub_config)
        except Exception as e:
            logger.error(f"ä¸‹è½½è®¢é˜…å¤±è´¥: {e}")
            sys.exit(1)

        # æ£€æŸ¥æ˜¯å¦æœ‰ proxies å­—æ®µ
        try:
            output = run_cmd(f"yq -r '.proxies' {sub_config}")
            if not output:
                raise Exception("è®¢é˜… proxies å­—æ®µä¸ºç©º")
        except Exception as e:
            logger.error(f"è§£æè®¢é˜…å¤±è´¥æˆ–æ—  proxies: {e}")
            sys.exit(1)

        logger.info("åˆå¹¶é…ç½®æ–‡ä»¶...")
        merged_yaml = run_cmd(
            f"yq eval-all 'select(fi == 0) *+ select(fi == 1) * select(fi == 2)' {rules_file} {sub_config} {default_config}"
        )
        config_file.write_text(merged_yaml)


def download_ui():
    ui_dir = MIHOMO_DIR / "ui"
    if ui_dir.exists():
        logger.info("UI å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
        return
    logger.info("ä¸‹è½½ UI...")
    run_cmd(
        f"git clone https://github.com/metacubex/metacubexd.git -b gh-pages --depth 1 {ui_dir}"
    )


def download_mmdb():
    mmdb_path = MIHOMO_DIR / "Country.mmdb"
    system_mmdb = Path("/etc/clash/Country.mmdb")
    if system_mmdb.exists():
        logger.info("æ£€æµ‹åˆ°ç³»ç»Ÿå·²å®‰è£… mmdb æ–‡ä»¶ï¼Œè·³è¿‡ä¸‹è½½")
        if mmdb_path.exists():
            mmdb_path.unlink()
        os.symlink(system_mmdb, mmdb_path)
        return

    temp_path = Path(tempfile.gettempdir()) / "Country.mmdb.temp"
    if not temp_path.exists():
        logger.info("ä¸‹è½½ mmdb æ–‡ä»¶...")
        url = "https://cdn.jsdelivr.net/gh/Dreamacro/maxmind-geoip@release/Country.mmdb"
        try:
            urllib.request.urlretrieve(url, temp_path)
        except Exception as e:
            logger.error(f"ä¸‹è½½ mmdb æ–‡ä»¶å¤±è´¥: {e}")
            sys.exit(1)
    os.rename(temp_path, mmdb_path)


# æ„é€ å¸¦ secret çš„ URL
def build_url(path, query=None):
    base = urllib.parse.urljoin(CLASH_HOST, path)
    query = query or {}
    if SECRET:
        query["secret"] = SECRET
    if query:
        return f"{base}?{urllib.parse.urlencode(query)}"
    return base


# å‘é€ GET è¯·æ±‚å¹¶è¿”å› JSON
def http_get_json(url):
    req = urllib.request.Request(url)
    with urllib.request.urlopen(req, timeout=10) as resp:
        return json.loads(resp.read().decode())


# å‘é€ PUT è¯·æ±‚
def http_put_json(url, data):
    body = json.dumps(data).encode()
    req = urllib.request.Request(url, data=body, method="PUT")
    req.add_header("Content-Type", "application/json")
    with urllib.request.urlopen(req, timeout=10) as resp:
        return resp.read().decode()


# è·å–ç­–ç•¥ç»„ä¸‹çš„æ‰€æœ‰èŠ‚ç‚¹å
def get_proxies():
    url = build_url("/proxies")
    data = http_get_json(url)
    proxies: dict[str, dict] = data.get("proxies", [])
    targets = []

    for name, proxy in proxies.items():
        if proxy.get("id") and proxy.get("name") and PROXY_SELECTOR.search(name):
            targets.append(proxy["name"])
    return targets


# æµ‹è¯•æŸä¸ªèŠ‚ç‚¹çš„å»¶è¿Ÿ
def get_delay(proxy_name: str):
    encoded = urllib.parse.quote(proxy_name.encode(), safe="")
    url = build_url(
        f"/proxies/{encoded}/delay",
        {"timeout": 5000, "url": "http://www.gstatic.com/generate_204"},
    )
    try:
        data = http_get_json(url)
        return data.get("delay", float("inf"))
    except:
        return float("inf")


# åˆ‡æ¢ç­–ç•¥ç»„ä½¿ç”¨çš„èŠ‚ç‚¹
def switch_proxy(proxy_name: str):
    encoded = urllib.parse.quote(TARGET_GROUP, safe="")
    url = build_url(f"/proxies/{encoded}")
    http_put_json(url, {"name": proxy_name})
    logger.info(f"âœ… å·²åˆ‡æ¢åˆ°èŠ‚ç‚¹ï¼š{proxy_name}")


def load_json(file_path: Path):
    """åŠ è½½ JSON æ–‡ä»¶"""
    if not file_path.exists():
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return {}
    with open(file_path, "r", encoding="utf-8") as f:
        return json.load(f)


class Cli:
    def __init__(self, sub_url: str = "", dir=MIHOMO_DIR):
        self.mihomo_dir = Path(dir).expanduser()
        self.sub_url = (
            sub_url
            or load_json(self.mihomo_dir / "subconfig.json").get("sub_url")
            or os.getenv("CLASH_SUB_URL")
        )

    def download(self, file: Literal["config", "ui", "mmdb", "all"]):
        """ä¸‹è½½é…ç½®ã€UI æˆ– mmdb æ–‡ä»¶"""

        sub_url = self.sub_url
        if not sub_url:
            raise ValueError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ CLASH_SUB_URL")
        actions = {
            "config": lambda: download_config(sub_url),
            "ui": download_ui,
            "mmdb": download_mmdb,
            "all": lambda: (download_config(sub_url), download_ui(), download_mmdb()),
        }
        actions[file]()

    def update_sub(self):
        """æ›´æ–°è®¢é˜…"""
        self.download("config")

    def run(self):
        """è¿è¡Œ Mihomo"""
        logger.info("å¯åŠ¨ mihomo...")
        run_cmd(f"mihomo -d {self.mihomo_dir}")

    def healthcheck(self):
        """æ£€æŸ¥ä¾èµ–"""
        logger.info("æ‰€æœ‰ä¾èµ–å·²æ»¡è¶³")
        missing_cmds = [cmd for cmd in REQUIRED_EXE if not shutil.which(cmd)]
        if missing_cmds:
            logger.error(f"ç¼ºå°‘ä¾èµ–: {', '.join(missing_cmds)}")
            sys.exit(1)

    def proxy(self, show: bool = False, auto: bool = False):
        """åˆ‡æ¢ä»£ç†èŠ‚ç‚¹"""
        proxies = get_proxies()

        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = {executor.submit(get_delay, proxy): proxy for proxy in proxies}
            delays = {future.result(): proxy for future, proxy in futures.items()}
            if delays and auto:
                min_delay = min(delays.keys())
                best_proxy = delays[min_delay]
                logger.info(f"æœ€ä½³èŠ‚ç‚¹: {best_proxy} (å»¶è¿Ÿ: {min_delay} ms)")
                switch_proxy(best_proxy)

            elif show:
                logger.info("å¯ç”¨èŠ‚ç‚¹åˆ—è¡¨:")
                for d, proxy in sorted(delays.items()):
                    logger.info(f"{proxy} - å»¶è¿Ÿ: {d} ms")


if __name__ == "__main__":
    try:
        fire_like(Cli)
    except Exception as e:
        logger.error(f"å‘ç”Ÿé”™è¯¯: {str(e)[:50]}")
        sys.exit(1)
